{
  "agent_id": "coder3",
  "task_id": "task_9",
  "files": [
    {
      "filename": "tests/test_rag_pipeline.py",
      "purpose": "Unit tests for RAG pipeline components",
      "priority": "medium",
      "dependencies": [
        "pytest",
        "pytest-asyncio",
        "unittest.mock"
      ],
      "key_functions": [
        "test_retrieval_accuracy",
        "test_response_generation",
        "test_error_handling",
        "test_edge_cases"
      ],
      "estimated_lines": 200,
      "complexity": "medium"
    },
    {
      "filename": "scripts/setup_environment.sh",
      "purpose": "Environment setup script for SMEs",
      "priority": "medium",
      "dependencies": [
        "bash",
        "python",
        "pip"
      ],
      "key_functions": [
        "install_dependencies",
        "setup_directories",
        "download_models",
        "validate_installation"
      ],
      "estimated_lines": 100,
      "complexity": "low"
    },
    {
      "filename": "docs/deployment_guide.md",
      "purpose": "Step-by-step deployment guide for SMEs",
      "priority": "medium",
      "dependencies": [],
      "key_functions": [],
      "estimated_lines": 300,
      "complexity": "low"
    }
  ],
  "project_info": {
    "project_name": "EASI_RAG_Industrial_SME_Knowledge_Assistant",
    "project_type": "nlp",
    "description": "Implementation of the EASI-RAG (Enterprise Application Support for Industrial RAG) method for deploying Retrieval-Augmented Generation tools in Small and Medium Enterprises. The system enables operators to query operational procedures using natural language and receive accurate, context-aware answers while maintaining low resource requirements and rapid deployment capabilities.",
    "key_algorithms": [
      "Retrieval_Augmented_Generation",
      "Hybrid_Dense_Sparse_Retrieval",
      "Hierarchical_Chunking",
      "BM25_Sparse_Retrieval",
      "Sentence_Transformer_Embeddings",
      "Pareto_Error_Analysis"
    ],
    "main_libraries": [
      "langchain",
      "sentence-transformers",
      "openai",
      "faiss-cpu",
      "pandas",
      "numpy",
      "scikit-learn",
      "pydantic",
      "fastapi",
      "streamlit",
      "python-docx",
      "openpyxl",
      "tiktoken",
      "ragas",
      "transformers"
    ]
  },
  "paper_content": "PDF: cs.CL_2508.21024v1_An-Agile-Method-for-Implementing-Retrieval-Augment.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nAn Agile Method for Implementing Retrieval -\nAugmented Generation Tools in Industrial \nSMEs  \n \nBOURDIN Mathieu*a,e, NEUMANN Anasb,c, PAVIOT Thomasa,d, \nPELLERIN Robertb,c, LAMOURI Samira,e \n \n*Corresponding author: mathieubourdin@hotmail.fr  \n \naLAMIH CNRS/Universit\u00e9 Polytechnique Hauts -de-France, Campus Mont Houy, 59313 Valenciennes Cedex 9, France  \nbCIRRELT, 2920 chemin de la Tour, pavillon Andr\u00e9 Aisenstadt, bureau 3520, 11290 Montr\u00e9al, Canada  \ncPolytechnique Montr\u00e9al, 2500 Chemin de Polytechnique Montr\u00e9al, 11290 Montr\u00e9al, Canada  \ndMeedia,  15 rue Glais Bizoin, 35000 Rennes, France  \neArts et M\u00e9tiers Institute of Technology, 151 boulevard de l\u2019H\u00f4pital, 75013 Paris, France  \n \nAbstract  \nRetrieval -Augmented Generation (RAG) has emerged as a powerful solution to mitigate the limitations \nof Large Language Models (LLMs), such as hallucinations and outdated knowledge. However, deploying \nRAG -based tools in Small and Medium Enterprises (SMEs) re mains a challenge due to their limited \nresources and lack of expertise in natural language processing (NLP). This paper introduces EASI -RAG : \nEnterprise Application Support for Industrial RAG, a structured, agile method designed to facilitate the \ndeployment  of RAG systems in industrial SME contexts. EASI -RAG is based on method engineering \nprinciples and comprises well -defined roles, activities  and techniques. The method was validated \nthrough a real -world case study in an environmental testing laboratory, where a RAG tool was \nimplemented to answer  operators\u2019 queries using data extracted from operational procedures . The \nsystem was deployed in less than a  month by a team with no prior RAG experience and was later \niteratively improved based on user s\u2019 feedback. Results demonstrate that EASI -RAG supports fast \nimplementation, high user adoption, delivers accurate answers , and enhances the reliability of \nunderlying data.  This work highlights the potential of RAG deployment in industr ial SMEs . Future works \ninclude  the need for generalization across diverse use cases and further integration with fine -tuned \nmodels.  \n \nKeywords:  Industry 4.0 ; Retrieval Augmented Generation (RAG) ; Natural Language Processing (NLP) ; Large \nLanguage Models (LLM) ; SME  \n  \n\n--- Page 2 ---\n1. Introduction  \nSince the release of OpenAI\u2019s ChatGPT at the end of 2022, public and research interest in language models has \nsurged dramatically. Within just a few years, numerous new language models have emerged, including Meta\u2019s \nLLaMa, Google\u2019s Gemini, OpenAI\u2019s GPT -4, Anthropic\u2019s Claude, Mistral AI\u2019s Mixtral, and the DeepSeek LLM family.  \nThese tools are increasingly adopted by companies to optimi se tasks that previously required significant manual \neffort. However, one notable limitation is their tendency to hallucinate [1]. Language models sometimes \ngenerate completely incorrect information . This issue poses a significant barrier to the use of LLMs across many \nbusiness applications that demand reliable information [2]. Furthermore, outputs from LLMs are not directly \nexplainable, as these models operate as black boxes due to their vast architectures containing billions of \nparameters. It has also been demonstrated that large language models are not well -suited for knowle dge \nstorage : Mousavi et al.  [3] show ed that among 24 LLMs tested, including some  recently released , none could \ncorrectly answer more than 80% of simple questions related to current events. This is because their \u201cknowledge\u201d \nreflects the data they were trained on and may therefore be outdated.  \nTo avoid these issues, it is necessary to integrate as much information as possible into the LLM before generating \nanswers. This can be done in two ways:  Retrieval -Augmented Generation (RAG) or finetuning . RAG  consists in \nretrieving relevant information from one or several databases linked to the query before generating a response \nusing the retrieved data. This approach  both reduces the risk of hallucination and allows for easier updating of \nsystem knowledge by maintaining a database rather than retraining an entire LLM  [4]. Fine -tuning , on the other \nhand, consists of  adapting a pre -trained language model to a specific domain or task by retraining it on a curated \ndataset relevant to the target application. This technique enables the model to internali se domain -specific \nterminology and patterns, improving performance on speciali sed queries.  \nBourdin et al.  [5] highlighted the lack of scientific literature offering guidance for deploying NLP solutions in \nindustrial context s. In particular, no existing guide helps industrial SMEs , which usually lack NLP expertise and \ngenerally have limited resources [6][7], navigate the wide range of available options for implementing previously \nmentioned solutions.  \nFine -tuning a language model requires specific domain -expertise as well as significant computational resources, \nwhich makes this option hardly compatible wit h SMEs, where both human and computational resources are \nusually more limited than in large companies  [6]. Moreover, using a fine -tuned LLM requires retraining every \ntime the underlying data is updated, which is a constraint for SMEs, whose agility is a competitive advantage \ncompared to larger enterprises. For these reasons, RAG -based solutions are much bette r suited for use in SMEs \nthan fine -tuning approaches.  \nThe objective of this work is to bridge the identified gap  in scientific literature  by developing a method  to \nimplemen t RAG -based solutions  within the context of industrial SMEs.  To this end, we constructed  the EASI -RAG \nmethod : Enterprise Application Support for Industrial RAG , an agile step -by-step method  for deploying RAG tools \nin industrial SMEs . The methodology proposed in this work was developed using an inductive approach based on \nan extensive review of the scientific literature. By synthesizing common patterns and principles from prior studies \non Retrieval -Augmented Generation (RAG) systems, we  derived a structured, step -by-step framework tailored \nfor industrial applications.  This review, initiated in a previous paper [5] and extended in the present work, forms \nthe basis for developing the proposed method . EASI -RAG  consider s the company\u2019s objectives as well as the  \nconstraint s and characteristics of its data to select the most appropriate RAG solutions for a given industrial  \napplication . \nThis work addresses Natural Language Processing (NLP) from two perspectives: both user queries and database \ndocuments are in natural language. It focuses on industrial use cases involving information synthesis or retrieval \nfrom text, such as maintenance, d iagnostics, customer support, regulatory compliance, summarization, and \nemployee training. The scope is limited to textual data , excluding  temporal data (e.g., time series forecasting) \nand numerical analysis. Although image -based data is not the primary fo cus, the method is compatible with tools \nsuch as  OCR. RAG is not required in small -context scenarios, as some LLMs now process over 100,000 tokens \nwithout preprocessing. However, Liu et al. [8] showed that too much context can impair retrieval, especially for \n\n--- Page 3 ---\nmid-context information , even at 2,000 tokens. RAG becomes valuable when the knowledge base exceeds \nseveral thousand tokens. The target use cases thus involve contexts where the volume of textual data justifies \nthe use of retrieval mechanisms.  \nThe model was subsequently tested and validated through a concrete RAG application case. The model was used \nto implement a system that uses RAG to answer operators\u2019 questions based on the company\u2019s operating \nprocedures. The system aims to save operators time when searching through dense documentation and improve \naccuracy by facilitating access to the correct inf ormation.  \nThe remainder  of the paper is  structured as follows: Section 2 reviews existing work in this field. Section 3 \npresents the EASI -RAG model. Section 4 then describes the application case developed with our industrial \npartner , followed by a result discussion in  Section 5 . Finally, Section 6 summari ses the main conclusions of this \nstudy.  \n2. Related work  \nSeveral studies explore the use of Retrieval -Augmented Generation (RAG) in industrial contexts. Chen et al.  [9] \nintroduce an interactive knowledge  management (IIKM)  system to assist technicians with technical repairs and \ninternal policy inquiries. Chaudhary et al. [10] design ed a Llama -based chatbot for Continuous Integration and \nContinuous Delivery ( CI/CD )-related questions in a telecom company, leveraging RAG for document -specific \naccuracy. Wan et al.  [11] developed a hybrid system combining Knowledge Graphs and vector retrieval for smart \nmanufacturing Q&A. L\u00f6whagen et al. [12] developed a chatbot leveraging Retrieval -Augmented Generation \n(RAG) to provide real -time responses to technicians\u2019 queries in the context of problem -solving  during train \ncommissioning . Du et al. [13] propose d LLM -MANUF, a multi -model framework that merges outputs from fine -\ntuned LLMs to enhance decision -making in manufacturing. Ren et al. [14] present ed RACI, a RAG -based model \nfor extracting causes from aviation accident reports, demonstrating high performance on noisy real -world data.  \nMa et al. [15] employ RAG  in combination with knowledge graphs to develop a reasoning tool aimed at \nsupporting complex fault diagnosis.  Lastly, Yang et al. [16] detail ed the deployment of a RAG -based virtual \nassistant (RAGVA) at Transurban, offering practical engineering insights and outlining eight key development \nchallenges. These examples highlight RAG\u2019s growing role in improving information access, decision support, a nd \nautomation across industrial sectors.  \nMore general papers offer broader analyses of RAG implementation. Cheng et al. [17] provide a comprehensive \nreview of RAG, examining its core components \u2014retrieval mechanisms, generation processes, and their \nintegration. They propose a taxonomy that classifies RAG approaches, from basic retrieval -augmented models \nto advanced systems incor porating multimodal inputs and reasoning capabilities. Similarly, Gao et al. [18] present \nan organi sed overview of the three key elements of RAG: retriever, generator, and augmentation techniques, \ndetailing the main technologies associated with each. Li et al. [19] examine the relationships between  generative \nAI usage and the performance of supply chain s in Chinese companie s. They d emonstrat e a positive correlation \nbetween the use of generative AI and supply chain performance, and provide practical guidance on implementing  \ngenerative AI across  supply chain s. Wamba et al. [20] adopt  a similar approach  with  English and American \ncompanies . They investigate the benefits  associated with Gen -AI in Operations and  Supply Chain \nManagement  and demonstrate that the integration of Gen -AI leads to the enhancement of the overall supply \nchain performance, particularly when coupled with  organizational learning. Finally, Wang et al. [21] explore \nexisting RAG methods and their potential combinations to identify optimal practices. Based on extensive \nexperimentation, they propose deployment strategies that effectively balance performance and efficiency.  \nFinally, some  articles address the implementation of emerging technologies in Small and Medium -sized \nEnterprises (SMEs) . Kong et al. [22] propose a Federated Learning framework to overcome the issues of a lack of \ndata to train Supply Chain Financing models (SCF)  in SMEs . Hanse n et al. [23] analysed 30 SMEs in their \ndigitalization process and developed a theoretical model of the determinants leading to smart manufacturing, \nwith a focus on the competencies required within the firm to successfully achieve digital transformation.  Zheng \net al. [24] analyse the different stages of digital transformation process  in SMEs  through interviews. Their work \nreveals  different phases of data -driven digital transformation in industrial SMEs and indicates preconditions for \n\n--- Page 4 ---\nstate  transitions in the context of manufacturing SMEs. Lastly, Battistoni et al. [25] analyse  how digital \ntechnologies support the development of information processing capabilities in Italian SMEs. They propose an \nIndustry 4.0 layer -based classification, providing managers and policymakers with insights on enabling effective \ndigital transformation.  \nHowever, although the previously cited papers cover parts of the topic we aim to address, none of them present \na method for implementing a RAG -based tool within industrial SME s. A method  is a \u201cgoal -oriented systematic \napproach, which helps to resolve theoretical and practical tasks\u201d [26]. As stated by Zellner, a method must \ninclude five mandatory elements: Activities, Techniques, Roles, Results, and Information Model [27]. The cited \npapers present either detailed solutions on specific application cases  or provide comprehensive descriptions of \nexisting RAG tools,  but without specifying how to choose among the various available options . Our contribution \ndistinguishes itself from cited works by presenting a generic and reusable method applicable to a wide range of \nRAG -based problem settings beyond a single use case , in the context of industrial SMEs.  \nTable 1  summari ses the aspects addressed by previous  works . None of the cited papers covers  the entire targeted \nscope. In particular, it is noteworthy that none of the papers specify the roles associated with each step. Existing \nmodels focus on the technical description of the implemented solutions but do not indicate the people who \nshould be in volved at each stage.  Paper  \nTitle  Mandatory Elements of a \nMethod  Context  Activities  \nTechniques  \nRoles  \nResults  \nInfo. model  \nRAG  \nIndustry  \nSMES  \n[9] Application of retrieval -augmented generation for interactive industrial \nknowledge management via a large language model  \u25d0 \u25cb \u25cb \u25cf \u25cb \u25cf \u25cf \u25cb \n[10] Developing a Llama -Based Chatbot for CI/CD Question Answering: A Case \nStudy at Ericsson  \u25cf \u25cf \u25cb \u25cf \u25cb \u25cf \u25cf \u25cb \n[11] Empowering LLMs by hybrid retrieval -augmented generation for domain -\ncentric Q&A in smart manufacturing  \u25cf \u25cf \u25cb \u25cf \u25cb \u25cf \u25cf \u25cb \n[12] Can a troubleshooting AI assistant improve task performance in industrial \ncontexts?  \u25cf \u25cf \u25cb \u25cf \u25cb \u25cf \u25cf \u25cb \n[13] LLM-MANUF: An integrated framework of Fine -Tuning large language \nmodels for intelligent Decision -Making in manufacturing  \u25cf \u25cf \u25cb \u25cf \u25d0 \u25cb \u25cf \u25cb \n[14] Retrieval -Augmented Generation -aided causal identification of aviation \naccidents: A large language model methodology  \u25cf \u25cf \u25cb \u25cf \u25cf \u25cf \u25d0 \u25cb \n[15] A knowledge -graph enhanced large language model -based fault diagnostic \nreasoning and maintenance decision support pipeline towards industry 5.0  \u25cf \u25cf \u25cb \u25cf \u25cb \u25cf \u25cf \u25cb \n[16] RAGVA: Engineering retrieval augmented generation -based virtual \nassistants in practice.   \u25cf \u25d0 \u25cb \u25cf \u25cf \u25cf \u25d0 \u25cb \n[17] A Survey on Knowledge -Oriented Retrieval -Augmented Generation  \u25cf \u25cf \u25cb \u25cf \u25cb \u25cf \u25cb \u25cb \n[18] Retrieval -Augmented Generation for Large Language Models: A Survey  \u25cf \u25cf \u25cb \u25cf \u25cb \u25cf \u25cb \u25cb \n[19] Generative AI -enabled supply chain management: The critical role of \ncoordination and dynamism  \u25d0 \u25d0 \u25d0 \u25cb \u25cb \u25cb \u25cf \u25cf \n[20] Are both generative AI and ChatGPT game changers for 21st -Century \noperations and supply chain excellence?  \u25d0 \u25d0 \u25d0 \u25cb \u25cb \u25cb \u25cf \u25cf \n[21] Searching for Best Practices in Retrieval -Augmented Generation  \u25cf \u25cf \u25cb \u25cf \u25cb \u25cf \u25cb \u25cb \n[22] A federated machine learning approach for order -level risk prediction in \nSupply Chain Financing  \u25cf \u25cf \u25cb \u25cf \u25cb \u25cb \u25cf \u25cf \n[23] Technology isn't enough for Industry 4.0: on SMEs and hindrances to digital \ntransformation  \u25cb \u25d0 \u25d0 \u25d0 \u25cb \u25cb \u25cf \u25cf \n[24] A dual evolutionary perspective on the Co -evolution of data -driven digital \ntransformation and value proposition in manufacturing SMEs  \u25cb \u25d0 \u25d0 \u25cf \u25cb \u25cb \u25cf \u25cf \n[25] Adoption paths of digital transformation in manufacturing SME  \u25cb \u25d0 \u25d0 \u25d0 \u25cb \u25cb \u25cf \u25cf \nLegend:          \n\u25cf = The paper fully addresses this aspect  \n\u25d0 = The paper partially addresses this aspect  \n\u25cb = The paper does not address this aspect at all  \nTable 1: Structured evaluation of related works  \n\n--- Page 5 ---\n3. EASI -RAG  method  \nThis section presents  the newly  proposed  EASI -RAG  approach : a method for implementing RAG -based tools in \nIndustrial SMEs. Following  Denner et al.'s definition of a method [28], if offers a  systematic procedure , including \nguidelines and  step -by-step  strateg ies, to implement solution . The proposed method is divided into five blocks \n(see Figure  1): \n\u2022 Initial design  (block 1) : Selection of technical solutions for each stage of the RAG process ; \n\u2022 Evaluation  (block 2) : Define the performance indicators and the targets , and perform initial evaluation ; \n\u2022 Error analysis and correction  (block 3) : analy se deviations from defined targets  and correct them ; \n\u2022 Integration  (block 4): Integrate the software into the production process ; and  \n\u2022 Feedback gathering  (block  5): Gather user feedback  and continuously improve the software . \n \n \n \nFigure  1 : General representation of the proposed method in five blocks: initial setup, individual evaluation, error correction, integration, test \nin real conditions, and feedback loops  \nIn the following sections , the  five blocks are broken down into activities. For each activity, techniques, roles, and \nresults  are specified . Finally, a  synthesis  of inputs and outputs is provided in the information model (Figure 2).  \nThe method proposed in this paper is based on Agile development principles [29]. The adopted strategy  is to \nquickly deliver an initial, functional RAG -based solution , then refine it iteratively  through  two iteration loops  \n(Figure 1) . This strategy  enables rapid tool deployment and strong adaptability  to the use case , which are key to \nSMEs [30]. It also allows to focus  on limiting components, saving time and resources , which is especially critical \nin the resource -constrained context of SMEs [6] [7]. Finally, t he method emphasi ses individuals and their \ninteractions  by indicating  which roles are required for each activity . The method defines four roles:  \n\u2022 User: End-user of the tool; if numerous, a representative sample should be involved;   \n\u2022 Data expert : individual with extensive knowledge of the data. Typically,  people  responsible for updating \nthe data and ensuring its validity ; \n\u2022 Process Owner  refers to t he individual responsible for the process affected by the implementation of \nthe tool , usually  the manager of the relevant department ; \n\u2022 Developer  refers to a  member of the IT team responsible for the development of the tool.  \nThis role -centric design addresses a gap in computer science research, which often focuses mainly on technical \naspects while overlooking user involvement (see Table 1, \u201cRelated Works\u201d).  \n\n\n--- Page 6 ---\n3.1 Initial Design  of the RAG Process  \nRAG process is typically broken down into several stages: data retrieval  and chunking, vectoris ation , chunk  \nretrieval , prompt engineering , and response generation.  \n3.1.1 Data retrieval  and chunking  \n \nThe first activity of our method is the data retrieval and chunking . This step should be  performed by  data experts \nand developers . This activity consists  in converting files into a usable format , cleaning the data , and segmenting \nthem into chunks.  While the first two can be carried out without any specific RAG knowledge, chunking  can be \nmore complex  due to the large number of available options . The most common  strategies  are fixed -size chunking  \n[4], element -based chunking  [31], sentence -based or paragraph -based  chunking  [21], sentence  window chunking  \n[32], recursive chunking [33], hierarchical chunking  [34], and semantic chunking  [35]. Comprehensive analysis \ncomparing different chunking methods exist in the literature [32] [34] [36], but a s an initial approach, we propose \nto segment the data simply based on their size .  \nFor short documents (e.g., maintenance reports, incident reports, emails), chunking is generally unnecessary; a \nsingle document can be treated as one chunk . For longer documents, chunking should primarily follow the \nsemantic structure  of the document  if possible  (sections and subsections; hierarchical chunking).  If not feasible, \nrecursive chunking  allows  to split documents by paragraphs  and sentences  until reaching a  given  token limit. In \nthis case, small chunks (64 -128 tokens) are preferable  for tasks involving short, fact -based answers whereas \nlarger chunks (512 \u20131024 tokens) are better suited for tasks  requiring descriptive or technical responses  [36]. \nThese methods allow  to preserv e semantic coherence and avoiding splitting ideas across chunks.   \n \n3.1.2 Vectoris ation  \n \nThe vectoris ation step should be performed by developers. Two broad families of vectoris ers are commonly used : \n\u2022 count -based methods , also called \u201csparse\u201d methods , such as TF -IDF or BM25, which represent \ndocuments based on term frequencies in a sparse vector  [37]; and  \n\u2022 dense vector approaches , also called \u201ce mbeddings \u201d, such as Sentence -BERT or E5, where documents \nare embedded into a continuous vector space using neural language models  [38]. \nSparse vectors enable precise keyword -based matching, particularly beneficial in industrial contexts where \ntechnical terminology is often crucial. In contrast, d ense vectors offer improved contextual understanding.  \n \nFor sparse vectoris ation, the most common approaches are TF -IDF and BM25. For dense vectoris ation , a wide \nrange of options is available, making the selection of the most  appropriate vectoris ation strategy complex [39]. \nOne resource that can help guide this decision is the Massive Text Embedding Benchmark (MTEB)  [40]. This \nbenchmark evaluates text embedding models across diverse datasets and tasks, including retrieval, making a \nvaluable starting point to identify candidates for RAG applications.  Once the candidates have been found, it is \nnecessary to test a few options  on its specific use case . However, vectoris er performance varies by dataset [39] \nand chunking strategy [34]. Testing multiple options  on the specific use case  is therefore important to select the \nmost suitable vectoris er. \n \n3.1.3 Retrieval of relevant chunks  \n \nStakeholders for the chunk retrieval definition  are data experts and developers.  The main  topic  here  is to define \nthe number of  chunks to retrieve . Finding the best balance in the number of retrieved chunks  is significant : \nretrieving too few chunks may result in the absence of essential contextual elements needed to answer the query \naccurately , conversely  providing an excessive amount of context  implies increased response times , higher \ncomputational resource consumption , and  can reduce the models  ability to find relevant information [34]. Even \nwith  \u201csmall\u201d  contexts of 2,000 tokens, language models can miss  some  information  depending on their location \nin the prompt  [8]. A good starting point to mitigate this risk is therefore to limit the context provided to the \nlanguage model to a few thousand tokens  at first  (2,000 \u20133,000 tokens). This amount can be increased later \ndepending on the language model used . However , keeping  an initial context size limited allows easier control \nover retrieved chunks and reduce s computational cost.  \n\n--- Page 7 ---\n3.1.4 Prompt engineering  \n \nStakeholders involved in the prompt engineering  activity  are users, data experts , and developers . Prompt \nengineering plays a critical role in obtaining high -quality outputs from large language models (LLMs).  \nTo achieve the most reliable responses, prompts should be formulated with clear and detailed instructions , \nideally providing an explicit example of the expected output  within the prompt  [41]. White et al. [42] present a \nprompt pattern  catalogue  to enhance LLM performance by adding simple instructions \u2014e.g., adopting a persona \nor rules to follow , to specify for example how to respond when lacking information or facing multiple plausible \nanswers.  For tasks that require to combine elements o r to reason  over the data, breaking down complex tasks \ninto sub -tasks can lead to better performance  [43] [44]. An easy technique to do so is  to include a phrase to \nexplicitly  ask for that  in the prompt  (for example, add \"Think step by step\"  in the prompt)  [43].  \n3.1.5 Language Model selection  \n \nOnly the developers and the process owner are involved at this stage. As some language models can be resource -\nintensive, the process owner must be involved in the case where investment in better hardware is required.  \nData confidentiality is crucial when selecting a language model, particularly in industrial companies. For sensitive \ncontent, local deployment is recommended to reduce the risk of data leakage or misuse [45]. SMEs , which  \ntypically operat e with fixed, limited infrastructure, should  prioriti se lightweight models with acceptable inference \ntimes  in this case . A practical approach  would  be to test first several LLMs in a cloud environment , using non -\nsensitive anonymi sed data, before  potentially  investi ng. If confidentiality is not an issue, proprietary models via \nAPIs are  more  cost-effective: high -quality models like GPT -4 Turbo or Claude 3 Haiku cost about $1 per million \ntokens  (August 2025) , making them  affordable even for SMEs.   \n \nOther criteria can be taken into consideration  when choosing  the language model:  \n\u2022 Context window : The model should support sufficiently long input s to capture relevant content ; \n\u2022 Training objectives : models finetuned on tasks similar to the use case  should be preferred ; and \n\u2022 Language /domain coverage: Models trained on corpora relevant to the industrial context (technical \nmanuals, engineering documentation) and in the same language should be preferred . \n \nTable 2 summarises the information about all the activities in the initial RAG pipeline design.  \n \nActivity  Roles  Step (s) Techniques  Outputs  \nData retrieval \nand chunking  Data experts  \nDevelopers  Accessing data  \nContent pruning  \nChunking  Element -based, \nhierarchical , recursive \nchunking  Available data in \nsegmented \nchunks  \nVectoris ation  Developers  Obtaining vector \nrepresentations of \nchunks  Sparse vectoris ations  \nDense vectoris ations  \nBenchmarks  and testing  Vector \nrepresentation of \nchunks  \nRetrieval of \nrelevant \nchunks  Data experts  \nDevelopers  Retrieving relevant \nchunks  for a given query  Real case testing  RAG model \nretriev ing X \nrelevant chunks  \nPrompt \nengineering  Users  \nData experts  \nDevelopers  Engineering the optimal \nprompt for desired \noutput  Provide explicit \nexamples  \nGive specific guidelines  Prompt template \nfor the language \nmodel  \nLanguage \nModel \nselection  Process owner  \nDevelopers  Choosing the language \nmodel for the RAG \npipeline  Choosing based on  \ncriteria  (confidentiality, \nwindow size, training , \u2026) Complete RAG \npipeline  \n(untested)  \nTable 2: Summary of the activities for the initial RAG pipeline design  \n \n\n--- Page 8 ---\n3.2. Evaluation of the RAG Process  \nAfter designing the initial RAG pipeline, it is crucial to define its evaluation, involving all key stakeholders: users, \ndata experts, process owners, and developers. The goal is to identify objectives and how to measure success.  \nEvaluation covers response quality \u2014whether the system answers correctly \u2014and performance aspects like \ninference time, response style, and cost.  \n \nTo assess response quality, the first step is to build a test query set. The test query set should cover all targeted \nquestion types (e.g., multimodal data, complex chains of thought) and include unanswerable questions to test \nhandling of unknowns. Linking test queries  to the document excerpt (s) required to answer  with the data experts \nis not mandatory, but is recommended to automate the pipeline evaluation . While automated generation of test \nqueries is  possible [46], manual query creation through user collaboration is recommended because  LLM are \nsensitive to phrasing [1]. Criteria for a good answer must be defined  as well : high recall (all expected info rmation ) \nand high precision (no additional  information ) are usual indicators , but are not the only indicator existing \n(maximum acceptable inference time, required response style, robustness to prompt variations, maximum \nresponse generation cost,  etc.). Some criteria  can be assessed via  automatic metrics (BLEU, ROUGE, BERTScore ), \nusually requiring reference answers  to compare with . However , automatic metrics  might  lack reliability in \nnuanced situations and lack  explainability , they are therefore recommended only  if the number of test cases is \nhigh  and alongside manual  evaluations.  \n \nAfter defining  all the relevant evaluation metrics, a performance target  must be set  for each criterion  and the \nRAG tool should be tested.  Table 3 summari ses the information for the RAG pipeline evaluation.  \n \nActivity  Roles  Step(s)  Techniques  Outputs  \nPerformance \ncriteria \ndefinition  Users  \nData experts  \nProcess owner  \nDevelopers  Defining the criteria to \nevaluate the \nimplemented pipeline on \nthe concrete use case  Manual evaluation \ncriteria  \nAutomatic tools (BLEU, \nROUGE, BertScore)  Performance \ncriteria  \nRAG pipeli ne \ntesting  Users  \nData experts  \nDevelopers  Perform initial \nevaluation  Evaluati on of  previously \ndefined performance \nindicators  Tested RAG \npipeline   \nTable 3: Summary of the activities for the RAG pipeline evaluation  \n3.3. Error analysis and correction  \nAt this stage, key performance indicators for the project have been identified and measured. The next step is to \nanaly se the gaps between actual performance and targets, and to implement the necessary corrections.  \n3.3.1 Error analysis   \nFirst, it is essential to examine the deviations from the established objectives. This analysis should be conducted \ncollaboratively by users, data experts, process owners, and developers.  \nThe initial priority is to assess the quality of the system\u2019s responses . Using the predefined test questions, each \nincorrect answer should be reviewed to pinpoint where the failure occurred in the RAG pipeline. Several \nquestions must be considered to this end: Was the information present in the documents? Was it in a supported \nformat? Did the model respond based on prior knowledge? Was the response style appropriate? And so forth.  \nTable 4 outlines all diagnostic questions  to locate potential  failure s (first three columns). Once all errors are \nreviewed, a Pareto analysis can hig hlight the most frequent issues to prioriti se corrective actions.  Note that a \nsingle error may involve multiple combined failures.  This step reflects the value of an agile approach: focusing \nfirst on the most critical weaknesses helps accelerate progress and avoid wasting time on already -satisfactory \ncomponents.  \n\n--- Page 9 ---\nRegarding the resolution of encountered issues, all individuals involved in the project may be potentially affected, \ndepending on the stage  at which our RAG tool is malfunctioning. The following paragraphs describe techniques \nthat can be employed to circumvent the various challenges that may arise during this analysis.  \n3.3.2 Incomplete data  / Data access issue   \nIf the data are incomplete or if the system encounters difficulties accessing certain types of data (e.g., figures or \ntables), several solutions can be considered. If the data are available as images, a multimodal RAG approach can \nbe employed to extract th e relevant information [47]. Additionally, recent studies have explored combining RAG \nwith query generation to retrieve data stored in SQL databases  [48]. \n3.3.3 Inadequate chunking  \nTo improve the performance of the chunking  component, basic approaches are to test alternate chunking \nstrategies  (recursive chunking, element -based, hierarchical chunking ) or to add  overlap  between chunks . More \nadvanced approaches involve dividing documents based on semantic similarity  (semantic chunking ) with another \nLLM  [49] or to combin e several  chunking strategies [50] [51]. \n3.3.4 Chunk retrieval  \nSeveral techniques can enhance retrieval performance.  Adjusting the context size (number and length of chunks)  \nis the main  lever , as too much or too little  context  can harm effectiveness. Testing or combining different types \nof vectorisers  or reranking  the top -k retrieved  chunks through a r elevance scoring method  can also improve \nretrieval  quality  [52]. Eventually, Hypothetical Document Embeddings  (HyDE ) can generate synthetic document \nrepresentations from queries  to improve retrieval accuracy  [53] and child -parent retrieving  can facilitate more \ncontext -aware retrieval by considering hierarchical relationships within the data  [54]. \n3.3.5 Unknown vocabulary  \nHandling unknown vocabulary is a common topic in industrial contexts.  For vocabulary unknown to an \nembedding model , an effective strategy is to implement hybrid retrieval [11], combining dense and sparse \nrepresentations  of texts : sparse vectors for keyword -based matching  and dense vectors for contextual \nunderstanding . Adding a vocabulary clarification step before the retrieval process  using a domain -specific \ndictionary  can also help  [55]. For vocabulary unknown to the language model , providing speciali sed vocabulary \nlists in the prompt or in  a separate document  in the data  can help . \n3.3.6 Answer based on prior knowledge  (hallucination)  \nResponses generated based on prior knowledge rather than the provided context \u2014commonly referred to as \nhallucinations \u2014pose significant challenges in RAG  systems.  Several techniques exist to mitigate this issue: ask  \nthe model to explicitly cite the document excerpt s used  to generate the answer  [42] [56], employ grounded \nmodels or models fine-tuned on question -answering , or incorporating a fact -checking step prior to delivering the \nfinal response [57]. \n3.3.7 Inadequacy relevance to the question  \nThis issue arises when the model misunderstands user intent. To overcome it, q uery rewriting can clarify  intent \nbefore retrieval and generation  [58]. Another solution can be to generate  multiple candidate answers and \nevaluating them with  a relevance metric , like RAGAS\u2019 answer relevance metric \u2014comparing artificial questions \nfrom answers to the real query \u2014to select the most accurate response  [59]. \n3.3.8 Lack of logical coherence  \nSeveral techniques have been proposed t o improve logical coherence in answers . Chain -of-Thought (CoT) \nprompting [60] guides the model to generate intermediate reasoning steps. Least -to-Most prompting [61] breaks \ndown complex problems into smaller, manageable subproblems  to solve sequentially in a coherent manner.  Plan -\nand-Solve [62] introduce an explicit planning phase before generation, allowing the model to outline a strategy \nto tackle the query.  \n\n--- Page 10 ---\n3.3.9 Lack of consistency in responses  \nLack of consistency in answers is another frequent issue when dealing with LLMs. Solutions to this issue include  \nlowering the  LM\u2019s temperature toward zero to produce more deterministic outputs [63]. Other approaches \ninclude refining the prompt to provide the most precise  and most explicit instructions possible to guide the model \ntoward consistent behavior  [42] or generating multiple responses for the same query and selecting  the most \nconsistent one [64]. \n3.3.10 Improper output format  \nCommonly recommended approach es to prevent poor output formatting are to specify the model\u2019s role and \ndesired response style within the prompt  [42]. Additionally, supplying examples of well -formatted responses for \nspecific questions  helps guide the model toward producing outputs that meet formatting expectations , as \ndemonstrated in [65]. \nTable 4 summari ses all possible actions to undertake based on the encountered  issues (last column).  \n Diagnostic Question  Issue  Metric for \nQuantification  Actions to correct  Retrieval  Are the data required \nto answer present in \nthe documents?  Incomplete \ndata  Manual \nanalysis  Complete the data  \nAdd new data sources  \nAre all retrieved \nchunks relevant?  Chunk \nretrieval  Context \nprecision*  Reduce  the amount of context ( quantity \nor size of retrieved chunks ); revise the \nvectoris ation method  \nAre all relevant \nchunks present?  Chunk \nretrieval  Context \nrecall*  Increase  the amount of context ; revise the \nvectoris ation method; use reranking \nstrategies ; add  child -parent retrieval  \nIs the information \naccessible to the \ntool? (table, image)  Data access \nissue  Manual \nanalysis  Implement multimodal RAG systems; use \nan agent model capable of executing \nmachine -readable actions  \nIs any excerpt split in \ntwo during chunking?  Inadequate \nchunking  Manual \nanalysis  Add overlap; revise chunking strategy ; \nconsider hybrid chunking approaches ; \nimplement child -parent retrieval  \nAre there terms \nunknown to the \nvectoriser  model?  Unknown \nvocabulary  Manual \nanalysis  Integrate sparse vectoris ation techniques; \nemploy a synonym dictionary to expand \nvocabulary coverage  Generation  Did the model \nrespond using prior \nknowledge?  Answer based \non prior \nknowledge  Faithfulness  Apply grounding techniques within the \nprompt; incorporate a fact -checking step \nprior to returning a response to the user  \nDid the answer \naddress the \nquestion ? Inadequate \nrelevance to \nthe question  Answer \nrelevance  Use a LM to reformulate the query; \ngenerate multiple answers  and pick best \none based on answer relevance  \nAre there terms \nunknown to the \nlanguage model?  Unknown \nvocabulary  Manual \nanalysis  Include relevant vocabulary lists directly in \nthe prompt to enhance the model's \nunderstanding  \nDid the LM lack  the \nlogic in the provided \nelements ? Lack of logical \ncoherence  Manual \nanalysis  Utilise structured prompting techniques \n(CoT, Least -to-Most prompting, Plan -and-\nSolve ) \nIs the quality of the \nobtained response \nrepeatable?  Lack of \nconsistency in \nresponses  Prompt \nagreement  Reduce the LM temperature;  \nrefine prompt; generate multiple answers \nand keep the most frequent one  \nIs the style or format \nof the response \nappropriate?  Improper \noutput \nformat  Manual \nanalysis  Refine prompt to specify the desired \nformat; provide examples of expected \nanswers  \n*Automatic analysis requires having identified in advance the specific chunks to be retrieved for each question  \nTable 4: Diagnostic questions for analysing incorrect responses of the RAG tool  \n\n--- Page 11 ---\nThe error analysis and correction activiti es are summarised in Table 5. \nActivity  Roles  Steps  Techniques  Outputs  \nError analysis  All Analy se the root cause(s) of all \nnon-achieved performance criteria  Many (see \nTable 4) Root cause analysis of \nnon-achieved targets  \nError \ncorrection  All Correcting the causes for not \nachieving the defined objectives  Many (see \nTable 4) Corrected RAG pipeline  \nTable 5: Summary of the error analysis and correction activiti es \n3.4. Integration into the production process  \nThe stakeholders involved at this stage include users, data experts, process owners, and developers, although \nnot all are required for every aspect. Due to the wide range of possible configurations, providing an exhaustive \nguide for this step is challenging. However, at a minimum, the following elements must be defined:  \n\u2022 Location of the RAG tool and access : in particular,  whether certain documents should be restricted to \nspecific user groups ; \n\u2022 Graphical user interface : should be designed collaboratively by developers and users to ensure usability \nand functionality ; \n\u2022 Document update s within the database : the frequency of updates  should be defined \u2014either at fixed \nintervals or manually triggered . This step typically involves by the process owner and data expert ; and  \n\u2022 Potential integrations with other tools . \n3.5. Feedback  gathering and continuous improvement  \nAt this stage, all stakeholders  are involved. The goal is to collect and leverage feedback to drive continuous \nsoftware improvement, following the A gile approach. Iteration loops facilitate testing and allow for rapid \nincorporation of results into subsequent development cycles, while also enhancing collaboration between \ndevelopers and designers [66]. Short, frequent feedback cycles guide both functional and ergonomic updates  \n[67]. Integrating real -time feedback into interactive systems also reinforce s user confidence, a critical aspect, \nespecially when introducing tools based on emerging technologies [68]. For these exchanges to be successful, \ncommunication must be bidirectional between users and developers , and not only from users to developers:  it \ndevelopers providing  inform ation to  users about  implemented changes is equally important  [69]. Additionally, \nassessing the impact of each incorrect answer with users and prioritise  based on both issue frequency and \nbusiness impact is critical in agile project management  [69]. In sum, the systematic collection and integration of \nuser feedback \u2014through an iterative and collaborative process \u2014is a key success factor in ensuring the adoption \nand scalability of a tool deployed in a professional environment.  \nAt this stage, if  new needs are identified by users, the process returns to the requirements definition stage, \nupdates the evaluation criteria accordingly, and proceeds again through all subsequent steps  (see Figure  1). \nTable 6 summarises the integration and feedback gathering activities.  \n\n--- Page 12 ---\nActivity  Roles  Steps  Techniques  Outputs  \nIntegration  Users  \nData experts  \nProcess owner  \nDevelopers  Integrate the designed \nRAG pipeline to the \nproduction  Depending on the use case  Operational RAG \nintegrated to \nproduction  \nFeedback \ngathering and \ncontinuous \nimprovement  Users  \nProcess owner  Defining how to collect \nand process feedback  Periodic collaborative \nreviews with short interval  \nPrioritization and \nstructuring of issues  Prioritised \nfeedback  \nTable 6: Summary of the integration and  feedback gathering activiti es \nFigure  2 presents the complete information model of the proposed  method.  \n \nFigure 2 :  Input -output  relationship diagram (information model)  \n\n\n--- Page 13 ---\n4. Industrial t est case  \nTo validate the relevance of the presented method, the EASI -RAG method  was applied to a real -world use case \nin an industrial company .  \n4.1 Company and data description  \nThe industrial partner is an environmental analysis laboratory in  France  employing around 200 people. The case \nstudy focuses on the development of a virtual assistant designed to answer employee questions based on the \ncompany\u2019s operating procedures.   \nThe team involved in the implementation of this virtual assistant consists of six members, distributed across the \npreviously described roles as follows:  \n\u2022 Users:  Two employees with different  age and seniority within the company  were chosen to represent a \n15-person team ; \n\u2022 Data Expert:  a senior  team member with over 20 years of experience  and extensive knowledge of the \noperating procedures ; \n\u2022 Data Owner:  the head of the concerned service ; and  \n\u2022 Developers:  two members of the IT team of the company , composed of five people . None of the m had \nprior experience with RAG  systems.  \nThe operating procedures used in this project consist of nine documents: seven Word files and two Excel \nspreadsheets. These documents vary in length and content . Some include images, while others are purely textual. \nMost documents are structured , but not all of them . Table 7 provides a summary of the characteristics of each \ndocument. For confidentiality reasons , document s are not disclosed.  \nDocument  \nnumber  Number of \ntokens  Number of \npages  Type of file  Structured  Includ es \npictures  \n1 950 5 Word  Yes Yes \n2 16947  95 Word  Yes Yes \n3 2389  10 Word  Yes Yes \n4 2458  2 Excel  - No \n5 2114  6 Word  Yes Yes \n6 2017  7 Word  Yes Yes \n7 1096  8 Word  Yes Yes \n8 945 2 Word  No No \n9 796 2 Excel  - No \nTotal  29712  137 - - - \nTable 7: Summary of the industrial data used in the use case  \n4.2 Initial Design of the RAG process  \nFollowing the EASI -RAG method , the  first st ep is a quick initial setup  of a RAG pipeline , that will be  later upgraded \nthrough several improvement loops . The initial components of our RAG pipeline are the following : \n\u2022 Data retrieval and chunking : Data are retrieved with standard retrieving tools (LangChain library1). No \npretreatment is  applied to data; notably , no preprocessing is applied to data before feeding them in the \nRAG pipeline.  Given that the data used contains both  structured and unstructured documents , the initial \nchunking  consists of a simple segmentation into fixed -size chunks of 1,000 tokens , without overlap.  \n\u2022 Vectorisation : Used data  contain little highly specific vocabulary , therefore, a dense vectorization is \nmore appropriate. The MTEB is then employed to search for an embedding model, meeting the \nfollowing criteria : a sufficiently small model size to embed all documents  within a reasonable time on a \ncompany\u2019s computer, and training oriented toward retrieval tasks. Once th e search is completed, \n \n1 https://python.langchain.com/docs/integrations/document_loaders/  \n\n--- Page 14 ---\nseveral models are selected and tested : the model with the highest MTEB score and a  reasonable  \nembedding time for the company\u2019s requirements is the all-MiniLM -L6-v22 model.  \n\u2022 Retrieval of relevant chunks : As the chunks are of 1,000 tokens each, the number of chunks initially \nretrieved is set to three, in order to limit the amount of context provided and to maintain better  control \nover the  retrieved  results at this stage.  \n\u2022 Prompt engineering : The initial prompt used  in this use case is intentionally simple: \"Using the following \ncontextual elements:  [list of retrieved chunks, separated by \" --------------- \"] respond to the following \nquery:  [insertion of the query] \u201d. \n\u2022 Language Model selection : As the data used does not contain any sensitive corporate information, the \nselected solution for the language model is \u201cgpt -3.5-turbo\u201d3, accessed through the OpenAI API . The \naverage cost  par query obtained with this model is approximately $0.0013 per query  (June 2025).  \n \n4.3 Evaluation of the RAG Process  \nOnce  the initial RAG pipeline is setup, the  six project participants jointly defined a set of validation criteria for \nassessing the effectiveness of the RAG system:  \n\u2022 The designed RAG pipeline operates on a \u201cstandard\u201d computer, namely one equipped with 8 GB of RAM \nand no GPU.  \n\u2022 The system produces responses almost instantaneously, with an inference time on the order of one \nsecond.  \n\u2022 A set of 42 test queries is then defined by the users , reflecti ng realistic questions that could assist them  \nin everyday situations by enabling faster retrieval of information. Eight additional questions , whose \nanswers are not present in the documents , are added to the set, resulting in a total of 50 test queries.  \nFor each test query, the data expert provides the expected answer and its location in the documents  to \nfacilitate the verification by the developers during subsequent stages.  \n\u2022 For each question, two performance criteria are defined : including  all expected elements (good recall)  \nand contain ing no incorrect information (good precision).  A response is considered correct if it satisfies \nboth criteria. Above all, the RAG pipeline must never  give instructions that contradict operational \nprocedures . Therefore, the following target values  are defined : \no More than 80% correct responses ; \no Fewer than 20% acceptable responses (partial responses or \u201cI don\u2019t know\u201d statements) ; and \no Zero responses that contradict operational manual instructions.  \nWith these criteria established, the team proceeds to evaluate the RAG pipeline against the defined targets.  The \ninitial evaluation of the RAG pipeline  produced the following outcomes:  \n\u2022 Average response time: approximately 2 seconds ; \n\u2022 17 correct answers ; \n\u2022 19 acceptable answers ; and \n\u2022 14 incorrect answers . \n4.5 Error analysis and correction  \nThe incorrect responses are subsequently analysed following the methodology outlined in Section 3. The analysis \nreveals ten distinct causes of error, which are summarised and illustrated in the following Figure 3. \n \n2 https://huggingface.co/sentence -transformers/all -MiniLM -L6-v2 \n3 https://platform.openai.com/docs/models/gpt -3.5-turbo  \n\n--- Page 15 ---\n \nFigure 3: Pareto analysis of  encountered  errors  after initial deploymen t \nThe cumulative number of error causes shown in the chart exceeds the total number of incorrect results, as \nmultiple issues  can contribute to a single incorrect outcome.  \nIdentified errors have been corrected through six successive modifications, addressing the issues from the most \nfrequent to the least frequent : \nMisunderstanding Excel data  - Document 4 : Without preprocessing, the chunks obtained from Document 4  were \nunusable:  the first chunk contained column headers and initial rows, while subsequent chunks  contained \nfollowing rows  without  headers . Additionally, chunks combined unrelated topics, which  led to poor semantic \nunderstanding of obtained  chunks and irrelevant retrievals. To address this, a preprocessing step was added  to \nsplit this document  into one row per chunk and prefix each cell\u2019s content with its column header.  \nVocabulary unknown by the embedding model:  Some user queries contained rare keywords that the embedder \neffectively \u201cignored\u201d by blending their representation with that of surrounding terms. As a result, retrieved \nchunks were off -topic, driven by similarity to non -relevant context words. To mitigate  this, a hybrid vectorization \napproach combining dense embeddings with sparse BM25 was implemented . The top three chunks from each \nmethod  are retrieved  for improved relevance.  \nMisunderstanding Excel Data (Document 9):  Similar to Document 4, data from Document 9 \u2014another Excel file \u2014\ncould not be retrieved. However, the previous solution failed here due to different structure s in documents : \nDocument 9 contains merged cells  and mostly consists in  empty content, with  information conveyed by  crosses \nor colours  in some cells . Given its brevity, the table was manually converted into a textual description  (one phrase \nfor each raw), allowing easier retrieval . \nChunking problem:  Some chunks were incomplete or split within key sentences, making retrieval more difficult. \nTo address this, documents were segmented according to their internal structure (sections and subsections) \nwhen possible, rather than using a fixed 1000 -token size. Only one document (Document 8) lacked such \nstructure . As it contained fewer than 1000 tokens, it was kept as a single chunk.  \nResponse hallucinated from parametric knowledge:  Some responses were not based on the information \ncontained in the retrieved chunks , but instead on the parametric knowledge of the language model. To address \nthis, the prompt was revised to include the following instruction at the end of each query: \u201cDo not use your prior \nknowledge; use only the information provided.\u201d  \nRelevant context, but important information missing:  In some cases, the RAG pipeline retrieves  only  chunks \nthat are relevant to the query but still miss other useful ones. This issue stem s from insufficient context. To \naddress it, the amount of context provided was increased  to retain  the top five chunks from each vectorisation \nmethod (sparse and dense).  \nThese various modifications resulted in the following outcomes:  \n\u2022 Average response time: approximately 2 seconds  (unchanged);  \n\u2022 44 correct answers;  \n\u2022 7 acceptable answers;  and \n\u2022 0 incorrect answers . \n\n\n--- Page 16 ---\nIt is important to emphasise that the implemented modifications do not necessarily produce uniformly positive \neffects across all queries. For example, increasing the amount of context can enhance the accuracy of certain \nresponses while inadvertently dimini shing that of others. In our case, the inclusion of the instruction \u201cDo not use \nyour prior knowledge; use only the information provided.\u201d  led to an overall improvement in answer quality  but \nalso resulted in a decline for three queries: the virtual assistant, which had previously returned the correct \nanswers, now responded with an explicit statement of uncertainty. This highlights the need to assess each change \nnot only in terms of aggregate performance gains, but also with regard to its potential adverse effects on \nindividual ca ses. \n4.7 Integration  and User Feedback  \nAfter implementing the previous corrections, the tool was deployed into production. A minimalist interface was \ncreated, and the tool was integrated into a software toolkit accessible to the team members.  A \u201cReport Incorrect \nAnswer\u201d  button was added to streamline feedback: when clicked, the query and its response are sent to the data \nexpert, who (1) provides the correct answer to the user, and (2) verifies whether the dataset contains it. If absent, \nthe dataset is supplemented and up dated; if present, the ca se is forwarded to developers for investigation.  \nA weekly feedback meeting was also established to review user comments systematically. After two weeks of \nproduction use, feedback indicated that the tool generally delivers helpful answers. However, users noted that \nresponses did not display the contextua l excerpts used, limiting their ability to verify or explore the information \nfurther. They recommended showing the relevant excerpt and source document. Interface improvements were \nalso suggested to enhance user experience.  Additionally, two new test questi ons\u2014both producing incorrect \nanswers \u2014were identified in the first week. These were added to the official test set to ensure that future \nenhancements address the issues effectively.  \nAll these elements, from the kickoff meeting to production deployment of the first version, were implemented \nwithin three weeks.  \n \n4.8 Overview and contributions  of the method  \nApplying the proposed method to a real -world case enabled the deployment of a RAG system in just a few weeks, \nusing developers with no prior RAG experience. The deployed system achieved 86% correct answers and \nproduced no unsatisfactory responses. Its agil e nature saved significant time by avoiding unnecessary complex \ntechniques and focusing on high -priority issues, allowing rapid delivery of a tool that supports the majority  of \ncases. Iterative improvements targeted critical blockers while avoiding effort on non -essential features.  \nSpecifically, this approach prevents the need in this case for: \n\u2022 Complex  chunking \u2014 a simple hierarchy -based method was sufficient , more advanced chunking techniques \n(semantic or combined chunking) were unnecessary ; \n\u2022 Multi -modal RAG \u2014 despite the presence of figures , testing showed that accessing image -based information \nwas not required since the text contained adequate explanations to answer questions  \n\u2022 Advanced retrieval techniques: while hybrid vectoris ation was needed to retrieve chunks linked to specific \nvocabulary terms, other sophisticated retrieval methods (e.g., Hypothetical Document Embedding, \nreranking, child -parent retrieving) were not; and  \n\u2022 Enhanced model reasoning \u2014 minor prompt adjustments sufficed without advanced reasoning strategies.  \nHardware costs were minimal (a few tens of euros for LLM usage). The main cost was human time: two part -time \ndevelopers over three weeks ( approximately 50 hours), plus about 5 hours each from users, the data expert, and \nthe process owner for meetings, totalling around 70 person -hours. This equates to one full -time employee for \ntwo weeks \u2014 a reasonable investment even for SMEs.  \nThe results show that the method enables efficient RAG deployment in SMEs by focusing resources on essential \nfeatures, delivering strong performance while keeping both complexity and cost low.  \n\n--- Page 17 ---\n5. Discussion  \nThe method proposed in this paper has proven to be both efficient and accessible, enabling the rapid deployment \nof a RAG system in a real -world setting using a small team with no experience in RAG technologies. Its agile and \npragmatic design allows for the  quick development of a functional tool that delivers correct answers in over 85% \nof cases  in a real industrial use case , without requiring advanced techniques such as complex chunking, \nmultimodal inputs, or sophisticated language model reasoning. By focusing on the most critical issues and \niterating based on user feedback, the method ensures relevant performance while mini mizing unnecessary \ndevelopment work. Additionally, the implementation cost \u2014especially in terms of hardware and human effort \u2014\nremains low, making this approach particularly suitable for SMEs looking to adopt AI solutions without significant \nresource investme nt. \nThe work presented in this paper presents certain limitations.  The proposed method has only been tested on a \nsingle real -world case, with specific data and particular stakeholders . Its applicability to other contexts with \ndifferent datasets or teams has not yet been explored, and broader validation across diverse applications and \nprofiles would be beneficial.  Moreover, t he study focuses solely on RAG techniques, excluding other NLP \nmethods such as fine -tuning language or embedding models, or the full RAG pi peline. While more complex and \nresource -intensive, these approaches can improve performance by incorporating domain -specific knowledge \nand adapting to varied document types. They were excluded due to SME constraints but remain valuable \nalternatives when RA G alone is insufficient.  \nImplementing RAG systems in industrial settings greatly enhances information access for staff and clients, \noffering key benefits: saving time by retrieving relevant data within seconds instead of manually searching large \ndocument volumes, and improving qua lity by uncovering answers that might otherwise remain unnoticed. These \ntools are particularly valuable for enterprises, especially when training new employees who require frequent \ninformation access. However, adoption requires careful monitoring of two ri sks: reduced interpersonal \ncommunication, potentially devaluing roles centered on knowledge sharing, and over -reliance on automated \nassistance, which may erode manual search skills. Maintaining some manual retrieval capabilities is essential to \npreserve ex pertise and prevent excessive dependency on the system.  \n6. Conclusion  \nThe EASI -RAG method presented in this paper describes an agile approach focused on the rapid deployment of \nRAG tools within the context of industrial SMEs. This method outlines the various activities required to \nimplement RAG tools, specifying for each acti vity the possible techniques, the personnel involved, and the \nexpected deliverables, following the framework of Zellner\u2019s methodology. A key aspect of this method is its agile \nnature, which involves quickly deploying an initial version of the tool and the n progressively improving it through \niterative feedback loops. The paper also proposes a list of techniques applicable depending on the challenges \nencountered during the initial implementation of a RAG tool.  \nThe proposed conceptual method was partially validated through a real -world application in an industrial SME. \nIts use enabled the rapid deployment of a RAG tool to answer operators\u2019 questions based on operational \nprocedures. Implementation was achieved at low cost, with a development team with no prior RAG expertise, \nwhile maintaining document confidentiality. Therefore, this method provides an ideal resource for SMEs , \nwhere flexibility is a major advantage and significant human or material investment may h inder the development \nof new tools. Furthermore, the continuous involvement of end -users throughout the project facilitated quick \nadoption by the production team.  \nFuture research could broaden validation of the EASI -RAG method across diverse SMEs to assess generalisability \nand robustness. Longitudinal studies should examine long -term effects on human factors, including \ncommunication and information -retrieval skills.  Integrating EASI -RAG with industrial systems (ERP, MES, \nmaintenance platforms) could enhance interoperability, while partially automating tasks such as document \ningestion, feedback analysis, and technique selection may reduce deployment time and effort. T hese \n\n--- Page 18 ---\ndevelopments would make the method more accessible to resource -constrained SMEs and strengthen its \npractical applicability.  \nAcknowledgements  \nThe authors would like to thank the industrial partner who provided access to the data and the real -world use \ncase that made this work possible. Their support and collaboration were instrumental in grounding our research \nin practical applications.  \nDeclaration  of Interest  \nThe authors declare that they have no conflict of interest, and that no financial, commercial, or personal \nrelationships influenced the design, execution, or reporting of this work.  \n7. References  \n[1] Beutel, G., Geerits, E., & Kielstein, J. T. (2023). Artificial hallucination: GPT on LSD?.  Critical Care , 27(1), 148.  \n[2] Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., ... & Fung, P. (2023). Survey of hallucination in natural language generation. ACM computing surveys, \n55(12), 1 -38. \n[3] Mousavi, S. M., Alghisi, S., & Riccardi, G. (2024). DyKnow: dynamically verifying time -sensitive factual knowledge in LLMs.  arXiv preprint \narXiv:2404.08700  \n[4] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., ... & Kiela, D. (2020). Retrieval -augmented generation for knowledge -intensive nlp \ntasks.  Advances in neural information processing systems , 33, 9459 -9474.  \n[5] Bourdin, M., Neumann, A., Paviot, T., Pellerin, R., & Lamouri, S. (2024). Exploring the applications of natural language processing and language models \nfor production, planning, and control activities of SMEs in industry 4.0: a systematic literature review.  Journal of Intelligent Manufacturing . \n[6] Moeuf, A., Tamayo, S., Lamouri, S., Pellerin, R., & Lelievre, A. (2016). Strengths and weaknesses of small and medium sized enterprises regarding the \nimplementation of lean manufacturing.  IFAC -PapersOnLine , 49(12), 71 -76. \n[7] Bauer, M., van Dinther, C., & Kiefer, D. (2020). Machine learning in SME: an empirical study on enablers and success factors.  \n[8] Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., & Liang, P. (2023). Lost in the middle: How language models use long \ncontexts.  arXiv preprint arXiv:2307.03172 . \n[9] Chen, L. C., Pardeshi, M. S., Liao, Y. X., & Pai, K. C. (2025). Application of retrieval -augmented generation for interactive industrial knowledge \nmanagement via a large language model.  Computer Standards & Interfaces , 94, 103995.  \n[10] Chaudhary, D., Vadlamani, S. L., Thomas, D., Nejati, S., & Sabetzadeh, M. (2024, October). Developing a Llama -Based Chatbot for CI/CD Question \nAnswering: A Case Study at Ericsson. In  2024 IEEE International Conference on Software Maintenance and Evolution (ICSME)  (pp. 707 -718). IEEE.  \n[11] Wan, Y., Chen, Z., Liu, Y., Chen, C., & Packianather, M. (2025). Empowering LLMs by hybrid retrieval -augmented generation for domain -centric Q&A in \nsmart manufacturing.  Advanced Engineering Informatics , 65, 103212.  \n[12] L\u00f6whagen, N., Schwendener, P., & Netland, T. (2025). Can a troubleshooting AI assistant improve task performance in industria l contexts?.  International \nJournal of Production Research , 1-22. \n[13] Du, K., Yang, B., Xie, K., Dong, N., Zhang, Z., Wang, S., & Mo, F. (2025). LLM -MANUF: An integrated framework of Fine -Tuning large language models \nfor intelligent Decision -Making in manufacturing.  Advanced Engineering Informatics , 65, 103263.  \n[14] Ren, T., Zhang, Z., Jia, B., & Zhang, S. (2025). Retrieval -Augmented Generation -aided causal identification of aviation accidents: A large language model \nmethodology.  Expert Systems with Applications , 278, 127306.  \n[15] Ma, Y., Zheng, S., Yang, Z., Pan, H., & Hong, J. (2025). A knowledge -graph enhanced large language model -based fault diagnostic reasoning and \nmaintenance decision support pipeline towards industry 5.0.  International Journal of Production Research , 1-22. \n[16] Yang, R., Fu, M., Tantithamthavorn, C., Arora, C., Vandenhurk, L., & Chua, J. (2025). RAGVA: Engineering retrieval augmented generation -based virtual \nassistants in practice.  arXiv preprint arXiv:2502.14930 . \n[17] Cheng, M., Luo, Y., Ouyang, J., Liu, Q., Liu, H., Li, L., ... & Chen, E. (2025). A survey on knowledge -oriented retrieval -augmented generation.  arXiv \npreprint arXiv:2503.10677 . \n[18] Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., ... & Wang, H. (2023). Retrieval -augmented generation for large language models: A survey.  arXiv \npreprint arXiv:2312.10997 , 2(1). \n[19] Li, L., Liu, Y., Jin, Y., Cheng, T. E., & Zhang, Q. (2024). Generative AI -enabled supply chain management: The critical role of coordination and \ndynamism.  International Journal of Production Economics , 277, 109388.  \n[20] Wamba, S. F., Queiroz, M. M., Jabbour, C. J. C., & Shi, C. V. (2023). Are both generative AI and ChatGPT game changers for 21 st-Century operations and \nsupply chain excellence?.  International Journal of Production Economics , 265, 109015.  \n[21] Wang, X., Wang, Z., Gao, X., Zhang, F., Wu, Y., Xu, Z., ... & Huang, X. (2024). Searching for best practices in retrieval -augmented generation.  arXiv \npreprint arXiv:2407.01219 . \n[22] Kong, L., Zheng, G., & Brintrup, A. (2024). A federated machine learning approach for order -level risk prediction in supply chain financing.  International \nJournal of Production Economics , 268, 109095.  \n[23] Hansen, A. K., Christiansen, L., & Lassen, A. H. (2024). Technology isn't enough for Industry 4.0: on SMEs and hindrances to digital \ntransformation.  International Journal of Production Research , 1-21. \n[24] ZHENG, J., ZHANG, J. Z., KAMAL, M. M., & MANGLA, S. K. (2025). A Dual Evolutionary Perspective on the Co -Evolution of Data -Driven Digital \nTransformation and Value Proposition in Manufacturing SMEs.  International Journal of Production Economics , 109561.  \n[25] Battistoni, E., Gitto, S., Murgia, G., & Campisi, D. (2023). Adoption paths of digital transformation in manufacturing SME.  International Journal of \nProduction Economics , 255, 108675.  \n\n--- Page 19 ---\n[26] Braun, C., Wortmann, F., Hafner, M., & Winter, R. ( 2005, March). Method construction -a core approach to organizational engineering. In  Proceedings of \nthe 2005 ACM symposium on Applied computing  (pp. 1295 -1299).  \n[27] Zellner, G. (2011). A structured evaluation of business process improvement approaches.  Business process management journal , 17(2), 203 -237. \n[28] Denner, M. S., P\u00fcschel, L. C., & R\u00f6glinger, M. (2018). How to exploit the digitalization potential of business processes.  Business & Information Systems \nEngineering , 60, 331 -349. \n[29] Manifesto, A. (2001). Manifesto for agile software development.  \n[30] Barzi, R. (2011). PME et agilit\u00e9 organisationnelle: \u00e9tude exploratoire.  Innovations , 35(2), 29 -45. \n[31] Yepes, A. J., You, Y., Milczek, J., Laverde, S., & Li, R. (2024). Financial report chunking for effective retrieval augmented  generation.  arXiv preprint \narXiv:2402.05131 . \n[32] P\u0142onka, M., Kocot, K., Ho\u0142da, K., Daniec, K., & Nawrat, A. (2025). A comparative evaluation of the effectiveness of document splitters for large language \nmodels in legal contexts.  Expert Systems with Applications , 126711.  \n[33] Br\u00e5dland, H., Goodwin, M., Andersen, P. A., Nossum, A. S., & Gupta, A. (2025, July). A New HOPE: Domain -agnostic Automatic Evaluation of Text \nChunking. In  Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval  (pp. 170 -179).  \n[34] Ahmed, T., & Choudhury, S. (2025). CHORUS: Zero -shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code.  arXiv \npreprint arXiv:2505.01485 .  \n[35] Qu, R., Tu, R., & Bao, F. (2024). Is semantic chunking worth the computational cost?.  arXiv preprint arXiv:2410.13070 . \n[36] Bhat, S. R., Rudat, M., Spiekermann, J., & Flores -Herr, N. (2025). Rethinking Chunk Size For Long -Document Retrieval: A Multi -Dataset Analysis.  arXiv \npreprint arXiv:2505.21700 . \n[37] Kuratomi, G., Pirozelli, P., Cozman, F. G., & Peres, S. M. (2025). A RAG -Based Institutional Assistant.  arXiv preprint arXiv:2501.13880 . \n[38] \u00c1lvaro, J. A. H., & Barreda, J. G. (2025). An advanced retrieval -augmented generation system for manufacturing quality control.  Advanced Engineering \nInformatics , 64, 103007.  \n[39] Caspari, L., Dastidar, K. G., Zerhoudi, S., Mitrovic, J., & Granitzer, M. (2024). Beyond benchmarks: Evaluating embedding model similarity for retrieval \naugmented generation systems.  arXiv preprint arXiv:2407.08275 . \n[40] Muennighoff, N., Tazi, N., Magne, L., & Reimers, N. (2022). MTEB: Massive text embedding benchmark.  arXiv preprint arXiv:2210.07316 . \n[41] Wang, X., Wang, Z., Gao, X., Zhang, F., Wu, Y., Xu, Z., ... & Huang, X. (2024). Searching for best practices in retrieval -augmented generation.  arXiv \npreprint arXiv:2407.01219 . \n[42] White, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., ... & Schmidt, D. C. (2023). A prompt pattern catalog to en hance prompt engineering with \nchatgpt.  arXiv preprint arXiv:2302.11382 . \n[43] Marvin, G., Hellen, N., Jjingo, D., & Nakatumba -Nabende, J. (2023, June). Prompt engineering in large language models. In  International conference on \ndata intelligence and cognitive informatics  (pp. 387 -402). Singapore: Springer Nature Singapore.  \n[44] Park, J., Atarashi, K., Takeuchi, K., & Kashima, H. (2025). Emulating Retrieval Augmented Generation via Prompt Engineering f or Enhanced Long Context \nComprehension in LLMs.  arXiv preprint arXiv:2502.12462 . \n[45] Chen, K., Zhou, X., Lin, Y., Feng, S., Shen, L., & Wu, P. (2025). A Survey on Privacy Risks and Protection in Large Language Models.  arXiv preprint \narXiv:2505.01976 . \n[46] Torres, J. J. G., B\u00eendil\u0103, M. B., Hofstee, S., Szondy, D., Nguyen, Q. H., Wang, S., & Englebienne, G. (2024). Automated Quest ion-Answer Generation for \nEvaluating RAG -based Chatbots. In  1st Workshop on Patient -Oriented Language Processing, CL4Health 2024  (pp. 204 -214). European Language \nResources Association (ELRA).  \n[47] Yu, S., Tang, C., Xu, B., Cui, J., Ran, J., Yan, Y., ... & Sun, M. (2024). Visrag: Vision -based retrieval -augmented generation on multi -modality \ndocuments.  arXiv preprint arXiv:2410.10594 . \n[48] Yu, X., Jian, P., & Chen, C. (2025). TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous Document Reasonin g. arXiv preprint \narXiv:2506.10380 . \n[49] Zhao, J., Ji, Z., Feng, Y., Qi, P., Niu, S., Tang, B., ... & Li, Z. (2024). Meta -chunking: Learning efficient text segmentation via logical perception. arXiv \npreprint arXiv:2410.12788.  \n[50] Verma, P. (2025). S2 Chunking: A Hybrid Framework for Document Segmentation Through Integrated Spatial and Semantic Analysis.  arXiv preprint \narXiv:2501.05485 . \n[51] Zhao, J., Ji, Z., Fan, Z., Wang, H., Niu, S., Tang, B., ... & Li, Z. (2025). MoC: Mixtures of Text Chunking Learners for Retr ieval -Augmented Generation \nSystem.  arXiv preprint arXiv:2503.09600 . \n[52] Ram, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton -Brown, K., & Shoham, Y. (2023). In -context retrieval -augmented language \nmodels.  Transactions of the Association for Computational Linguistics , 11, 1316 -1331.  \n[53] Gao, L., Ma, X., Lin, J., & Callan, J. (2023, July). Precise zero -shot dense retrieval without relevance labels. In  Proceedings of the 61st Annual Meeting of \nthe Association for Computational Linguistics (Volume 1: Long Papers)  (pp. 1762 -1777).  \n[54] Huang, H., Huang, Y., Yang, J., Pan, Z., Chen, Y., Ma, K., ... & Cheng, J. (2025). Retrieval -Augmented Generation with Hierarchical Knowledge.  arXiv \npreprint arXiv:2503.10150 . \n[55] An, Z., Ding, X., Fu, Y. C., Chu, C. C., Li, Y., & Du, W. (2024). Golden -Retriever: High -Fidelity Agentic Retrieval Augmented Generation for Industrial \nKnowledge Base.  arXiv preprint arXiv:2408.00798 . \n[56] \u0141ajewska, W., & Balog, K. (2025). Ginger: Grounded information nugget -based generation of responses.  arXiv preprint arXiv:2503.18174 . \n[57] Tran, H., Wang, J., Ting, Y., Huang, W., & Chen, T. (2024). LEAF: Learning and Evaluation Augmented by Fact -Checking to Improve Factualness in Large \nLanguage Models.  arXiv preprint arXiv:2410.23526 . \n[58] Ma, X., Gong, Y., He, P., Zhao, H., & Duan, N. (2023, December). Query rewriting in retrieval -augmented large language models. In  Proceedings of the \n2023 Conference on Empirical Methods in Natural Language Processing  (pp. 5303 -5315).  \n[59] Es, S., James, J., Anke, L. E., & Schockaert, S. (2024, March). Ragas: Automated evaluation of retrieval augmented generation . In Proceedings of the 18th \nConference of the European Chapter of the Association for Computational Linguistics: System Demonstrations  (pp. 150 -158).  \n[60] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., ... & Zhou, D. (2022). Chain -of-thought prompting elicits reasoning in large language \nmodels.  Advances in neural information processing systems , 35, 24824 -24837.  \n[61] Zhou, D., Sch\u00e4rli, N., Hou, L., Wei, J., Scales, N., Wang, X., ... & Chi, E. (2022). Least -to-most prompting enables complex reasoning in large language \nmodels.  arXiv preprint arXiv:2205.10625 . \n\n--- Page 20 ---\n[62] Wang, L., Xu, W., Lan, Y., Hu, Z., Lan, Y., Lee, R. K. W., & Lim, E. P. (2023). Plan-and-solve prompting: Improving zero -shot chain -of-thought reasoning \nby large language models.  arXiv preprint arXiv:2305.04091 . \n[63] Peeperkorn, M., Kouwenhoven, T., Brown, D., & Jordanous, A. (2024). Is temperature the creativity parameter of large language  models?.  arXiv preprint \narXiv:2405.00492 . \n[64] Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., ... & Zhou, D. (2022). Self -consistency improves chain of thought reasoning in language \nmodels.  arXiv preprint arXiv:2203.11171 . \n[65] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). Language models are few -shot learners.  Advances in \nneural information processing systems , 33, 1877 -1901.  \n[66] Ferreira, J., Noble, J., & Biddle, R. (2007, August). Agile development iterations and UI design. In  Agile 2007 (AGILE 2007)  (pp. 50 -58). IEEE.  \n[67] Ma, Y., Lu, Y., & Saparova, D. (2014). The role of iterative usability evaluation in agile development: a case study.  \n[68] Honeycutt, D., Nourani, M., & Ragan, E. (2020, October). Soliciting human -in-the-loop user feedback for interactive machine learning reduces user trust \nand impressions of model accuracy. In  Proceedings of the AAAI Conference on Human Computation and Crowdsourcing  (Vol. 8, pp. 63 -72). \n[69] Li, Z. S., Arony, N. N., Devathasan, K., Sihag, M., Ernst, N., & Damian, D. (2024, February). Unveiling the life cycle of use r feedback: Best practices from \nsoftware practitioners. In  Proceedings of the 46th IEEE/ACM International Conference on Software Engineering  (pp. 1 -13).",
  "project_dir": "artifacts/projects/EASI_RAG_Industrial_SME_Knowledge_Assistant",
  "communication_dir": "artifacts/projects/EASI_RAG_Industrial_SME_Knowledge_Assistant/.agent_comm",
  "assigned_at": "2025-08-30T20:45:13.051899",
  "status": "assigned"
}